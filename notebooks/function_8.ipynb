{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626ab6f7-60b5-4acd-99bd-f89dce5c070c",
   "metadata": {},
   "source": [
    "## Function Description\n",
    "You’re optimising an eight-dimensional black-box function, where each of the eight input parameters affects the output, but the internal mechanics are unknown. \n",
    "\n",
    "The objective is to find the parameter combination that maximises the function’s output, such as performance, efficiency or validation accuracy. Because the function is high-dimensional and likely complex, global optimisation is hard, so identifying strong local maxima is often a practical strategy.\n",
    "\n",
    "For example, imagine you’re tuning an ML model with eight hyperparameters: learning rate, batch size, number of layers, dropout rate, regularisation strength, activation function (numerically encoded), optimiser type (encoded) and initial weight range. Each input set returns a single validation accuracy score between 0 and 1. The goal is to maximise this score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc5417-49b8-47df-8e69-63b81e2e2d68",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdc0322-5a90-46b6-a594-6ee6482ea95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original n: 40\n",
      "\n",
      "X:\n",
      " [[0.60499445 0.29221502 0.90845275 0.35550624 0.20166872 0.57533801\n",
      "  0.31031095 0.73428138]\n",
      " [0.17800696 0.56622265 0.99486184 0.21032501 0.32015266 0.70790879\n",
      "  0.63538449 0.10713163]\n",
      " [0.00907698 0.81162615 0.52052036 0.07568668 0.26511183 0.09165169\n",
      "  0.59241515 0.36732026]\n",
      " [0.50602816 0.65373012 0.36341078 0.17798105 0.0937283  0.19742533\n",
      "  0.7558269  0.29247234]\n",
      " [0.35990926 0.24907568 0.49599717 0.70921498 0.11498719 0.28920692\n",
      "  0.55729515 0.59388173]\n",
      " [0.77881834 0.0034195  0.33798313 0.51952778 0.82090699 0.53724669\n",
      "  0.5513471  0.66003209]\n",
      " [0.90864932 0.0622497  0.23825955 0.76660355 0.13233596 0.99024381\n",
      "  0.68806782 0.74249594]\n",
      " [0.58637144 0.88073573 0.74502075 0.54603485 0.00964888 0.74899176\n",
      "  0.23090707 0.09791562]\n",
      " [0.76113733 0.85467239 0.38212433 0.33735198 0.68970832 0.30985305\n",
      "  0.63137968 0.04195607]\n",
      " [0.9849332  0.69950626 0.9988855  0.18014846 0.58014315 0.23108719\n",
      "  0.49082694 0.31368272]\n",
      " [0.11207131 0.43773566 0.59659878 0.59277563 0.22698177 0.41010452\n",
      "  0.92123758 0.67475276]\n",
      " [0.79188751 0.57619134 0.69452836 0.28342378 0.13675546 0.27916186\n",
      "  0.84276726 0.62532792]\n",
      " [0.1435503  0.93741452 0.23232482 0.00904349 0.41457893 0.40932517\n",
      "  0.55377852 0.2058408 ]\n",
      " [0.76991655 0.45875909 0.55900044 0.69460444 0.50319902 0.72834638\n",
      "  0.78425353 0.66313109]\n",
      " [0.05644741 0.06595555 0.02292868 0.03878647 0.40393544 0.80105533\n",
      "  0.48830701 0.89308498]\n",
      " [0.86243745 0.48273382 0.2818694  0.54410223 0.88749026 0.38265469\n",
      "  0.60190199 0.47646169]\n",
      " [0.3515119  0.59006494 0.9094363  0.67840835 0.21282566 0.08846038\n",
      "  0.410153   0.19572429]\n",
      " [0.73590364 0.03461189 0.72803027 0.14742652 0.29574314 0.44511731\n",
      "  0.97517969 0.37433978]\n",
      " [0.68029397 0.25510465 0.86218799 0.13439582 0.3263292  0.28790687\n",
      "  0.43501048 0.36420013]\n",
      " [0.04432925 0.01358149 0.25819824 0.57764416 0.05127992 0.15856307\n",
      "  0.59103012 0.07795293]\n",
      " [0.77834548 0.75114565 0.31414221 0.90298577 0.33538166 0.38632267\n",
      "  0.74897249 0.9887551 ]\n",
      " [0.89888711 0.5236417  0.87678325 0.21869645 0.90026089 0.28276624\n",
      "  0.91107791 0.47239822]\n",
      " [0.14512029 0.11932754 0.42088822 0.38760861 0.15542283 0.87517163\n",
      "  0.51055967 0.72861058]\n",
      " [0.33895442 0.56693202 0.3767511  0.09891573 0.65945169 0.24554809\n",
      "  0.76248278 0.73215347]\n",
      " [0.17615002 0.29396143 0.97567997 0.79393631 0.92340076 0.03084229\n",
      "  0.80325452 0.59589758]\n",
      " [0.02894663 0.02827906 0.48137155 0.6131746  0.67266045 0.02211341\n",
      "  0.6014833  0.52488505]\n",
      " [0.19263987 0.63067728 0.41679584 0.49052929 0.79608602 0.65456706\n",
      "  0.27624119 0.29551759]\n",
      " [0.94318502 0.21885062 0.72118408 0.42459707 0.986902   0.53518298\n",
      "  0.71474318 0.96009372]\n",
      " [0.5327214  0.8336926  0.071399   0.11681148 0.73069311 0.93737559\n",
      "  0.86650798 0.127902  ]\n",
      " [0.44709584 0.84395253 0.72954612 0.63915138 0.40928714 0.13264569\n",
      "  0.03590888 0.44683847]\n",
      " [0.38222497 0.55713584 0.85310163 0.33379569 0.26572127 0.48087292\n",
      "  0.23764706 0.76863196]\n",
      " [0.53281953 0.86230848 0.53826712 0.04944293 0.71970119 0.9067059\n",
      "  0.10823094 0.52534791]\n",
      " [0.39486519 0.33180167 0.7407543  0.69786172 0.73740444 0.78377681\n",
      "  0.25449546 0.87114551]\n",
      " [0.98594539 0.87305363 0.07039262 0.05358729 0.73415296 0.52025852\n",
      "  0.81104004 0.10336036]\n",
      " [0.96457339 0.97397979 0.66375335 0.66221599 0.67312167 0.90523762\n",
      "  0.45887462 0.5609175 ]\n",
      " [0.47207071 0.16820264 0.08642757 0.45265551 0.48061922 0.62243949\n",
      "  0.92897446 0.11253627]\n",
      " [0.85600695 0.6388937  0.32619202 0.66850311 0.24029837 0.21029889\n",
      "  0.16754636 0.96358986]\n",
      " [0.81003174 0.63504604 0.26954758 0.86960534 0.66192159 0.25225873\n",
      "  0.76567003 0.89054867]\n",
      " [0.79625252 0.00703653 0.35569738 0.48756605 0.74051962 0.7066501\n",
      "  0.99291449 0.38173437]\n",
      " [0.48124533 0.10246072 0.21948594 0.67732237 0.24750919 0.24434086\n",
      "  0.16382453 0.71596164]\n",
      " [0.082603   0.246903   0.063949   0.005541   0.89156    0.065065\n",
      "  0.03421    0.790226  ]\n",
      " [0.028856   0.116572   0.039282   0.013514   0.908145   0.128256\n",
      "  0.080534   0.261035  ]\n",
      " [0.254479   0.178111   0.1433     0.174452   0.990812   0.902001\n",
      "  0.331054   0.50629   ]\n",
      " [0.023181   0.200365   0.023147   0.3178     0.989006   0.378693\n",
      "  0.04237    0.298744  ]\n",
      " [0.033651   0.368316   0.079935   0.359407   0.978995   0.405521\n",
      "  0.224425   0.175438  ]\n",
      " [0.028702   0.406025   0.006107   0.261502   0.979774   0.631485\n",
      "  0.212759   0.379693  ]\n",
      " [0.007883   0.092914   0.044324   0.224486   0.997791   0.389709\n",
      "  0.269241   0.542502  ]\n",
      " [0.019908   0.009382   0.231172   0.234328   0.993311   0.654482\n",
      "  0.139657   0.545297  ]\n",
      " [0.006673   0.165494   0.167964   0.346417   0.997175   0.745173\n",
      "  0.237118   0.135109  ]\n",
      " [0.003463   0.025076   0.078187   0.211108   0.958656   0.866822\n",
      "  0.191692   0.895037  ]\n",
      " [0.118033   0.096534   0.036537   0.119078   0.994273   0.486326\n",
      "  0.172532   0.274793  ]\n",
      " [0.245039   0.019064   0.040008   0.438195   0.982569   0.053781\n",
      "  0.017728   0.992977  ]\n",
      " [0.020417   0.262283   0.13775    0.176284   0.975896   0.387017\n",
      "  0.216025   0.318099  ]]\n",
      "\n",
      "y:\n",
      " [7.3987211  7.00522736 8.45948162 8.28400781 8.60611679 8.54174792\n",
      " 7.32743458 7.29987205 7.95787474 5.59219339 7.85454099 6.79198578\n",
      " 8.97655402 7.3790829  9.598482   8.15998319 7.13162397 6.76796253\n",
      " 7.43374407 9.01307515 7.31089382 5.84106731 9.14163949 8.81755844\n",
      " 6.45194313 8.83074505 9.34427428 6.88784639 8.04221254 7.69236805\n",
      " 7.92375877 8.42175924 8.2780624  7.11345716 6.40258841 8.47293632\n",
      " 7.97768459 7.46087219 7.43659353 9.18300525 9.70409658 9.76136669\n",
      " 9.73531581 9.83190454 9.8478371  9.82717652 9.91055683 9.87944753\n",
      " 9.83552    9.79798029 9.93818592 9.53576473 9.93715931]\n",
      "\n",
      "n:  53\n",
      "\n",
      "current maximum:\n",
      "n: 51\n",
      "y: 9.9381859226016\n",
      "X: [0.118033 0.096534 0.036537 0.119078 0.994273 0.486326 0.172532 0.274793]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "from scipy.stats.qmc import Sobol\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load original dataset\n",
    "X = np.load(\"../data/function_8/initial_inputs.npy\")\n",
    "y = np.load(\"../data/function_8/initial_outputs.npy\")\n",
    "d = X.shape[1] # dimension\n",
    "\n",
    "print(f\"original n: {len(X)}\")\n",
    "print()\n",
    "\n",
    "# week 1 = initial\n",
    "\n",
    "# week 2\n",
    "x_new = np.array([[0.082603, 0.246903, 0.063949, 0.005541, 0.891560, 0.065065, 0.034210, 0.790226]])\n",
    "y_new = np.array([9.7040965785564])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 3\n",
    "x_new = np.array([[0.028856, 0.116572, 0.039282, 0.013514, 0.908145, 0.128256, 0.080534, 0.261035]])\n",
    "y_new = np.array([9.761366687093])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 4\n",
    "x_new = np.array([[0.254479, 0.178111, 0.143300, 0.174452, 0.990812, 0.902001, 0.331054, 0.506290]])\n",
    "y_new = np.array([9.735315806578])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 5\n",
    "x_new = np.array([[0.023181, 0.200365, 0.023147, 0.317800, 0.989006,\n",
    "                   0.378693, 0.042370, 0.298744]])\n",
    "y_new = np.array([9.83190454446054])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 6\n",
    "x_new = np.array([[0.033651, 0.368316, 0.079935, 0.359407,\n",
    "                   0.978995, 0.405521, 0.224425, 0.175438]])\n",
    "y_new = np.array([9.8478371028301])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 7\n",
    "x_new = np.array([[0.028702, 0.406025, 0.006107, 0.261502,\n",
    "                   0.979774, 0.631485, 0.212759, 0.379693]])\n",
    "y_new = np.array([9.8271765150661])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 8\n",
    "x_new = np.array([[0.007883, 0.092914, 0.044324, 0.224486,\n",
    "                   0.997791, 0.389709, 0.269241, 0.542502]])\n",
    "y_new = np.array([9.9105568314181])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 9\n",
    "x_new = np.array([[0.019908, 0.009382, 0.231172, 0.234328,\n",
    "                   0.993311, 0.654482, 0.139657, 0.545297]])\n",
    "y_new = np.array([9.8794475320086])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 10\n",
    "x_new = np.array([[0.006673, 0.165494, 0.167964, 0.346417,\n",
    "                   0.997175, 0.745173, 0.237118, 0.135109]])\n",
    "y_new = np.array([9.8355199980514])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 11\n",
    "x_new = np.array([[0.003463, 0.025076, 0.078187, 0.211108, 0.958656, 0.866822, 0.191692, 0.895037]])\n",
    "y_new = np.array([9.797980289198101])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 12\n",
    "x_new = np.array([[0.118033, 0.096534, 0.036537, 0.119078, 0.994273, 0.486326, 0.172532, 0.274793]])\n",
    "y_new = np.array([9.9381859226016])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# week 13\n",
    "x_new = np.array([[0.245039, 0.019064, 0.040008, 0.438195, 0.982569, 0.053781, 0.017728, 0.992977]])\n",
    "y_new = np.array([9.5357647305826])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "# final submission\n",
    "x_new = np.array([[0.020417, 0.262283, 0.137750, 0.176284, 0.975896, 0.387017, 0.216025, 0.318099]])\n",
    "y_new = np.array([9.9371593056499])\n",
    "\n",
    "X = np.vstack((X, x_new))\n",
    "y = np.concatenate((y, y_new))\n",
    "\n",
    "print(\"X:\\n\", X)\n",
    "print()\n",
    "print(\"y:\\n\", y)\n",
    "print()\n",
    "print(\"n: \", len(y))\n",
    "print()\n",
    "idx_best = np.argmax(y)\n",
    "print(f\"current maximum:\\nn: {idx_best+1}\\ny: {y[idx_best]}\\nX: {X[idx_best]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ff369-19f0-41bf-9aa5-6fffe046ce68",
   "metadata": {},
   "source": [
    "## Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ed0cb4-9e60-4f44-a2b7-8aab3ff97837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__constant_value is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next point to query: 0.044221-0.145154-0.090362-0.041203-0.975990-0.576365-0.123078-0.265991\n"
     ]
    }
   ],
   "source": [
    "# GP setup\n",
    "kernel = (ConstantKernel(1.0, (1e-2, 1e2)) *\n",
    "          Matern(length_scale=np.ones(d), nu=1.5) +\n",
    "          WhiteKernel(noise_level=5e-3, noise_level_bounds=(1e-6, 1e-1)))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel,\n",
    "                              n_restarts_optimizer=8,\n",
    "                              normalize_y=True,\n",
    "                              random_state=42)\n",
    "gp.fit(X, y)\n",
    "\n",
    "# Sobol candidates in [0,1]^8\n",
    "sob = Sobol(d=d, scramble=True, seed=None)\n",
    "C = sob.random_base2(m=18)\n",
    "\n",
    "# GP predictions\n",
    "mu, sigma = gp.predict(C, return_std=True)\n",
    "\n",
    "# Expected Improvement (EI)\n",
    "y_best = np.max(y)\n",
    "xi = 0.01              \n",
    "imp = mu - y_best - xi\n",
    "Z = imp / sigma\n",
    "ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "\n",
    "# Pick next query\n",
    "x_next = C[np.argmax(ei)]\n",
    "print(\"Next point to query:\",\n",
    "      \"-\".join(f\"{x:.6f}\" for x in x_next))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
